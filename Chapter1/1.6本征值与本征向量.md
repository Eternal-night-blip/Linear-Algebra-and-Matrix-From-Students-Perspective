## 1.6本征值与本征向量  

&emsp;&emsp;上一节中我们讲到有时候需要将复杂的矩阵化成简单的矩阵，那么什么是简单的矩阵，它要满足什么性质呢？举个例子看看吧。假设线性变换$T$在自然基下的表示为：  

$$\begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}$$  

&emsp;&emsp;它的作用是将自然基$ij$分别转换成$(3,0)$，$(1,2)$。似乎这个矩阵看起来也很简单，但是我们很容易发现，自然基$j$在变换后脱离了原来的方向。我们希望找出这样一组基$\hat{i}\hat{j}$，在经历过线性变换$T$的作用后，新的基$\hat{i’}\hat{j’}$与旧的基$\hat{i}\hat{j}$仍在同一直线上。那么很自然地列出约束条件以求得这样的$\hat{i}\hat{j}$:  

$$
T\begin{bmatrix} \hat{i} & \hat{j} \end{bmatrix}
= \begin{bmatrix} T\hat{i} & T\hat{j} \end{bmatrix}
= \begin{bmatrix} \lambda_1\hat{i} & \lambda_2 \hat{j}
\end{bmatrix}
$$  

&emsp;&emsp;也就是：  

$$
\begin{bmatrix} 3 & 1\\ 0 & 2\end{bmatrix}
\begin{bmatrix} x_{\hat{i}} \\ y_{\hat{i}} \end{bmatrix}
=\begin{bmatrix} \lambda_1 x_{\hat{i}} \\ \lambda_1 y_{\hat{i}} \end{bmatrix}
$$

$$
\begin{bmatrix} 3 & 1\\ 0 & 2\end{bmatrix}
\begin{bmatrix} x_{\hat{j}} \\ y_{\hat{j}} \end{bmatrix}
=\begin{bmatrix} \lambda_2 x_{\hat{j}} \\ \lambda_2 y_{\hat{j}} \end{bmatrix}
$$

&emsp;&emsp;这里不考虑计算过程，直接得出结果，在自然系下$\hat{i}=(1,0)$，$\hat{j}=(-1,1)$，$\lambda_1=3$，$\lambda_2=2$。其中这样的向量成为本征向量(eigenvector),对应的伸缩因子$\lambda$称为本征值(eigenvalue)[^footnote1]。那么线性变换$T$在$\hat{i}\hat{j}$下的矩阵就是  

$$
\begin{bmatrix} 1 & - 1 \\ 0 & 1 \end{bmatrix}^{-1}
\begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}
\begin{bmatrix} 1 & - 1 \\ 0 & 1 \end{bmatrix}
=\begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix}
$$

&emsp;&emsp;发现了什么！线性变换$T$在特征向量构成的坐标系下的矩阵是对角矩阵[^footnote2]，对角线上的元素分别是两个本征向量对应的本征值，这并不是巧合，因为我们要找的就是这样的矩阵！线性变换在本征基下的矩阵变得简单了，只有对角线上有非零元素。那么对于多次线性变换，复合后的线性变换异常简单。  

$$
\begin{bmatrix} 3 & 0\\0 & 2\end{bmatrix}^{n}
=\begin{bmatrix} 3^n & 0 \\0 & 2^n \end{bmatrix}
$$  &emsp;&emsp;

&emsp;&emsp;但是如果在自然基下呢？你会感到十分麻烦，计算机处理起来也会很吃力。  

$$
\begin{bmatrix} 3 & 1\\0 & 2\end{bmatrix}^{n}
=?
$$  

&emsp;&emsp;所以线性变换在本征基组成的坐标系下的表示会很容易处里。但是我得告诉你一个不幸的事实，不是所有的线性变换都有本征值与本征向量，就算有，也不一定能组成一个二维坐标系（加入原坐标系是二维的），可能只有一个方向的本征基。另外我们希望本征基相互之间最好是正交的，且长度为1,有这样的矩阵吗？当然是有的，那么新的分解就是谱分解(Spectrum Decomposition)，也就谱理论。当然了还有大家耳熟能详的奇异值分解(SVD, Singular Value Decomposition)，也与本征值本征向量有关。  

&emsp;&emsp;特别说明，我们一般是把线性变换在自然基下的矩阵分解成新基下的矩阵，也就是:  

$$
\begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}
=\begin{bmatrix} 1 & - 1 \\ 0 & 1 \end{bmatrix}
\begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix}
\begin{bmatrix} 1 & - 1 \\ 0 & 1 \end{bmatrix}^{-1}
$$  [^footnote3]

[^footnote1]: 大部分教科书把他叫做特征向量与特征值，这里采用*Linear Algebra Done Right*的中文版对它的叫法，原因在于机器学习里，我们会有叫做特征向量(feature vector)的东西，但很明显两者不是同一个东西，而且笔者认为本征似乎更能凸显eigenvector的意蕴。  

[^footnote2]: 对角矩阵是指，从左上到右下的对角线（主对角线）外的其他元素均为0。至于主对角线上的元素，为零不为零，则没有要求。  

[^footnote3]: 一般形式就是$A=X\Lambda X^{-1}$， $A$是线性变换在自然基下的矩阵，$X$的列是自然系下表示的本征基，$\Lambda$是对角矩阵，对角线上的本征值与$X$相应列的本征基相匹配。  

